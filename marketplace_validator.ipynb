{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad04831",
   "metadata": {},
   "source": [
    "# Marketplace Validator\n",
    "- **Goal**: Validate lender listings (credit cards, loans) against Marketplace Standards and suggest compliant rewrites.\n",
    "\n",
    "- **Process**:\n",
    "\n",
    "    1. Load the baseline prompt and sample listings.\n",
    "\n",
    "    2. Call a Validator LLM (Gemini) per listing; save structured JSON outputs.\n",
    "\n",
    "    3. Call a Judge LLM to score each validator output against the standards.\n",
    "\n",
    "    4. Aggregate into a leaderboard with pass rate and criterion scores.\n",
    "\n",
    "    4. Swap in an improved prompt and compare results.\n",
    "\n",
    "#### Assumptions and constraints\n",
    "- **Standards compliance** is assessed via an LLM (not a deterministic rules engine).\n",
    "\n",
    "- **Self-evaluation bias**: Same model family may be used for validation and judging; acceptable for fast iteration, but cross-model judging is preferable for final scoring.\n",
    "\n",
    "- **Input shape**: Listings are plain text; the prompt template contains a {{ listing }} placeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adbb39d",
   "metadata": {},
   "source": [
    "## Imports and installs\n",
    "\n",
    "The code  imports standard libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a327d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-genai --user\n",
    "# !pip install -q google-generativeai pandas tqdm python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89682c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from typing import Optional, Any, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd86c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "os.environ['GEMINI_API_KEY'] = 'AIzaSyC4Y2gm1beWZx8tMVJNnZjgE_dB1ac-zbM' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b0b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.errors import APIError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0facc3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "logger = logging.getLogger(__name__) \n",
    "logger.setLevel(logging.INFO) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f839ad",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "- `read_txt` reads a single text file (used for prompts).\n",
    "\n",
    "- `write_file` saves the results of a validation run (standards prompt, original listing, and AI output) into a JSON file for later analysis.\n",
    "\n",
    "- `read_listings` reads all `.txt` files in a given directory (`data/sample_listings/`) to get the credit card listings for validation.\n",
    "\n",
    "- `load_client` initializes the `genai.Client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_txt(file_path: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Reads the entire content of a text file and returns it as a string.\n",
    "\n",
    "    This function attempts to open and read a file specified by `file_path`.\n",
    "    It uses a context manager (`with open(...)`) to ensure the file is properly\n",
    "    closed even if errors occur.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The full path to the text file to be read.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The entire content of the file as a single string, or\n",
    "                     None if a FileNotFoundError occurs (e.g., the file\n",
    "                     doesn't exist at the specified path).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 'r' mode opens the file for reading\n",
    "        with open(file_path, 'r') as file:\n",
    "            # read() reads the entire content of the file\n",
    "            file_content = file.read()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.info(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return None\n",
    "    return file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec03f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(path: str, result_prompt: Any, listing: Any, text: Any) -> None:\n",
    "    \"\"\"\n",
    "    Writes a dictionary containing structured data to a file in JSON format.\n",
    "\n",
    "    Args:\n",
    "        path (str): The full path to the output file (e.g., 'results/data.json').\n",
    "                    The file will be overwritten if it already exists ('w' mode).\n",
    "        result_prompt (Any): Data to be stored under the \"standards\" key. \n",
    "                             This typically represents a set of expected standards or criteria.\n",
    "        listing (Any): Data to be stored under the \"listing\" key.\n",
    "                       This often represents a list of items or configuration details.\n",
    "        text (Any): Data to be stored under the \"actual_output\" key.\n",
    "                    This usually represents the raw or processed output from a process.\n",
    "\n",
    "    Returns:\n",
    "        None: The function handles file writing and logging internally.\n",
    "    \"\"\"\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\n",
    "            \"standards\": result_prompt,\n",
    "            \"listing\": listing,\n",
    "            \"actual_output\": text\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    logger.info(f\"Wrote to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ed889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_listings(sample_listings_directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads the content of all text files (*.txt) within a specified directory.\n",
    "\n",
    "    Args:\n",
    "        sample_listings_directory_path (str): The path to the directory \n",
    "                                              containing the text files.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list where each element is the string content of one \n",
    "                   successfully read text file. Files that cause errors are \n",
    "                   skipped, and their content is not included in the list.\n",
    "    \"\"\"\n",
    "    listings = []\n",
    "    file_paths = glob.glob(os.path.join(sample_listings_directory_path, '*.txt'))\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            # Open the file in read mode ('r')\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                listings.append(content)\n",
    "                logger.info(f\"Successfully read: {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "            logger.info(f\"Error: File not found at {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"Error reading {file_path}: {e}\")\n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324989b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_client():\n",
    "    \"\"\"\n",
    "    Initializes and returns the Gemini API client.\n",
    "\n",
    "    Returns:\n",
    "        genai.Client or None: An initialized instance of the Gemini client \n",
    "                              if successful, otherwise returns None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the client. Assumes GEMINI_API_KEY is set in the environment.\n",
    "        client = genai.Client()\n",
    "        return client\n",
    "    except Exception:\n",
    "        logger.info(\"ERROR: Could not initialize Gemini Client. Ensure GEMINI_API_KEY is set.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f0141",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed44797",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_listings_directory_path = 'data/sample_listings/'\n",
    "prompt_path = 'prompts/marketplace_validator_prompt.txt'\n",
    "ROOT = \"data/validation_original_prompt\"   \n",
    "MODEL = \"gemini-2.5-flash\"                 \n",
    "OUT_CSV = \"leaderboard.csv\"\n",
    "OUT_JSON = \"details.json\"\n",
    "MAX_RETRIES = 3\n",
    "TEMPERATURE = 0\n",
    "model = MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f0bf8",
   "metadata": {},
   "source": [
    "## Inputs   \n",
    "\n",
    "**Data Preparation**: The original prompt is read into the `prompt` variable, and the sample listings are loaded into the `listings` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read original prompt \n",
    "prompt = read_txt(prompt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c795cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read: data/sample_listings\\listing_01.txt\n",
      "Successfully read: data/sample_listings\\listing_02.txt\n",
      "Successfully read: data/sample_listings\\listing_03.txt\n",
      "Successfully read: data/sample_listings\\listing_04.txt\n",
      "Successfully read: data/sample_listings\\listing_06.txt\n",
      "Successfully read: data/sample_listings\\listing_07.txt\n",
      "Successfully read: data/sample_listings\\listing_08.txt\n",
      "Successfully read: data/sample_listings\\listing_09.txt\n",
      "Successfully read: data/sample_listings\\listing_10.txt\n",
      "Successfully read: data/sample_listings\\listing_11.txt\n",
      "Successfully read: data/sample_listings\\listing_12.txt\n",
      "Successfully read: data/sample_listings\\listing_13.txt\n",
      "Successfully read: data/sample_listings\\listing_14.txt\n",
      "Successfully read: data/sample_listings\\listing_15.txt\n",
      "Successfully read: data/sample_listings\\listing_16.txt\n",
      "Successfully read: data/sample_listings\\listing_17.txt\n",
      "Successfully read: data/sample_listings\\listing_18.txt\n",
      "Successfully read: data/sample_listings\\listing_19.txt\n",
      "Successfully read: data/sample_listings\\listing_20.txt\n",
      "Successfully read: data/sample_listings\\listing_21.txt\n",
      "Successfully read: data/sample_listings\\listing_22.txt\n",
      "Successfully read: data/sample_listings\\listing_23.txt\n",
      "Successfully read: data/sample_listings\\listing_24.txt\n",
      "Successfully read: data/sample_listings\\listing_25.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the listings\n",
    "listings = read_listings(sample_listings_directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce34c61",
   "metadata": {},
   "source": [
    "## Evaluate the listings with prompt\n",
    "\n",
    "This section executes the marketplace validation using the original, provided prompt (`prompts/marketplace_validator_prompt.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_listings(listings: list[str], prompt_template: str, output_filepath: str = \"data/\"):\n",
    "    \"\"\"\n",
    "    Loops through a list of credit card listings, generates a prompt for each, \n",
    "    calls the Gemini API for validation, and writes the results to a separate, \n",
    "    uniquely named JSON file.\n",
    "\n",
    "    Args:\n",
    "        listings (list[str]): A list where each element is the content of a listing file.\n",
    "        prompt_template (str): The template string containing the validation standards and the \n",
    "                                placeholder '{{ listing }}'.\n",
    "        output_filepath (str): This is used as the base path for the output directory \n",
    "                                (e.g., 'data/').\n",
    "    \"\"\"\n",
    "    client = load_client()\n",
    "\n",
    "    logger.info(f\"Starting API validation for {len(listings)} listings...\")\n",
    "\n",
    "    # Determine the output directory from the base path\n",
    "    output_dir = os.path.dirname(output_filepath)\n",
    "    if not output_dir:\n",
    "        # If output_filepath is just \"data/\", os.path.dirname returns empty, so use \"data\"\n",
    "        output_dir = output_filepath.rstrip('/')\n",
    "\n",
    "    for i, listing in enumerate(listings):\n",
    "        logger.info(f\"\\n--- Processing Listing {i+1}/{len(listings)} ---\")\n",
    "\n",
    "        # Prepare prompt\n",
    "        result_prompt = prompt_template.replace('{{ listing }}', listing)\n",
    "        \n",
    "        ai_response_text = \"API_CALL_FAILED\"\n",
    "        max_retries = 5\n",
    "        wait_time = 2  # Exponential backoff start time\n",
    "\n",
    "        # Get response with exponential backoff\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.info(f\"  Attempt {attempt + 1}: Calling Gemini...\")\n",
    "                \n",
    "                response = client.models.generate_content(\n",
    "                    model=\"gemini-2.5-flash\",\n",
    "                    contents=result_prompt\n",
    "                )\n",
    "                \n",
    "                ai_response_text = response.text\n",
    "                logger.info(f\"  SUCCESS! Response received.\")\n",
    "                break  # Exit retry loop on success\n",
    "                \n",
    "            except APIError as e:\n",
    "                # Handle transient errors (e.g., rate limits)\n",
    "                if attempt < max_retries - 1:\n",
    "                    logger.info(f\"  API Error ({e}). Retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    wait_time *= 2  # Double wait time\n",
    "                else:\n",
    "                    logger.info(\"  Max retries reached. Saving error message and proceeding.\")\n",
    "                    ai_response_text = f\"API_ERROR_MAX_RETRIES: {e}\"\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Handle unexpected, non-retryable errors\n",
    "                logger.info(f\"  UNEXPECTED ERROR: {e}. Skipping this listing.\")\n",
    "                ai_response_text = f\"UNEXPECTED_ERROR: {e}\"\n",
    "                break\n",
    "        \n",
    "        # Write to a unique file for this listing\n",
    "        unique_filename = f\"listing_validation_{i+1}.json\"\n",
    "        unique_filepath = os.path.join(output_dir, unique_filename)\n",
    "\n",
    "        try:\n",
    "            # We now call the newly defined write_file function\n",
    "            write_file(unique_filepath, result_prompt, listing, ai_response_text)\n",
    "            logger.info(f\"  Results for Listing {i+1} saved.\")\n",
    "        except Exception as e:\n",
    "             # Catching general exception here in case of a file system error\n",
    "             logger.info(f\"FATAL ERROR saving results: {e}\")\n",
    "             return\n",
    "\n",
    "    logger.info(\"\\n--- Validation Loop Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656857d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API validation for 24 listings...\n",
      "\n",
      "--- Processing Listing 1/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 1 saved.\n",
      "\n",
      "--- Processing Listing 2/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 2 saved.\n",
      "\n",
      "--- Processing Listing 3/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 3 saved.\n",
      "\n",
      "--- Processing Listing 4/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 4 saved.\n",
      "\n",
      "--- Processing Listing 5/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 5 saved.\n",
      "\n",
      "--- Processing Listing 6/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 6 saved.\n",
      "\n",
      "--- Processing Listing 7/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 7 saved.\n",
      "\n",
      "--- Processing Listing 8/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 8 saved.\n",
      "\n",
      "--- Processing Listing 9/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 9 saved.\n",
      "\n",
      "--- Processing Listing 10/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 10 saved.\n",
      "\n",
      "--- Processing Listing 11/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 11 saved.\n",
      "\n",
      "--- Processing Listing 12/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 12 saved.\n",
      "\n",
      "--- Processing Listing 13/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 13 saved.\n",
      "\n",
      "--- Processing Listing 14/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 14 saved.\n",
      "\n",
      "--- Processing Listing 15/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 15 saved.\n",
      "\n",
      "--- Processing Listing 16/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 16 saved.\n",
      "\n",
      "--- Processing Listing 17/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 17 saved.\n",
      "\n",
      "--- Processing Listing 18/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 18 saved.\n",
      "\n",
      "--- Processing Listing 19/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 19 saved.\n",
      "\n",
      "--- Processing Listing 20/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 20 saved.\n",
      "\n",
      "--- Processing Listing 21/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 21 saved.\n",
      "\n",
      "--- Processing Listing 22/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 22 saved.\n",
      "\n",
      "--- Processing Listing 23/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 23 saved.\n",
      "\n",
      "--- Processing Listing 24/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 24 saved.\n",
      "\n",
      "--- Validation Loop Complete ---\n"
     ]
    }
   ],
   "source": [
    "process_listings(listings, prompt, output_filepath = \"data/validation_original_prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dadf2be",
   "metadata": {},
   "source": [
    "## LLM as a judge\n",
    "\n",
    "Evaluates the results using the LLM-as-a-Judge framework.\n",
    "\n",
    "- The `JUDGE_SYSTEM` prompt is loaded from a file (`prompts/judge_prompt.txt`).\n",
    "\n",
    "- The `build_user_prompt` function prepares a structured context (standards, listing, actual output) for the judge model.\n",
    "\n",
    "- The `call_gemini_judge` function submits the structured context, along with the judge's system prompt, to the Gemini model for evaluation. It uses `temperature: 0` for deterministic scoring and includes retry logic. It expects a JSON response with \"verdict\" and \"scores\".\n",
    "\n",
    "- The `evaluate_folder` function orchestrates the judge process: it finds all output files from the `process_listings` step, calls the `judge_fn` (`call_gemini_judge`) for each one, and compiles the results into a detailed record DataFrame (`df_a`) and an aggregated leaderboard (`lb_a`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e67d4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_SYSTEM = read_txt(\"prompts/judge_prompt.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b231f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_prompt(standards: str, listing: str, actual_output: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs a structured context string for the LLM by embedding key data \n",
    "    fields after JSON-serializing them.\n",
    "\n",
    "    Args:\n",
    "        standards (str): The prompt defining the criteria or expected output \n",
    "                         (the \"standards\").\n",
    "        listing (str): The raw input content being analyzed (the \"listing\").\n",
    "        actual_output (str): The result or previous output generated by a process.\n",
    "\n",
    "    Returns:\n",
    "        str: A single formatted string containing the three inputs under \n",
    "             a \"Context:\" header, with each input value safely enclosed \n",
    "             by JSON serialization.\n",
    "    \"\"\"\n",
    "    # Keep it compact & safe-JSON\n",
    "    return (\n",
    "        \"Context:\\n\"\n",
    "        f'\"standards\": {json.dumps(standards)}\\n'\n",
    "        f'\"listing\": {json.dumps(listing)}\\n'\n",
    "        f'\"actual_output\": {json.dumps(actual_output)}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini_judge(standards: str, listing: str, actual_output: str,\n",
    "                      retries: int = MAX_RETRIES):\n",
    "    \"\"\"\n",
    "    Calls the Gemini model (acting as a 'judge') to evaluate and score\n",
    "    the provided data against defined standards, incorporating retry logic.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        standards (str): The criteria or scoring rubric used for the evaluation.\n",
    "        listing (str): The raw input content being evaluated.\n",
    "        actual_output (str): The result or proposed improvement generated in a\n",
    "                            previous step.\n",
    "        retries (int, optional): The maximum number of attempts to call the API.\n",
    "                                Defaults to MAX_RETRIES (assumed global).\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The parsed and validated JSON response payload from the \n",
    "                        model, expected to contain \"verdict\" and \"scores\" keys.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Re-raises the last exception if all retry attempts fail\n",
    "                (e.g., API error, JSONDecodeError, or dictionary key error).\n",
    "    \"\"\"\n",
    "\n",
    "    user_presult_prompt  = build_user_prompt(standards, listing, actual_output)\n",
    "    full_prompt = JUDGE_SYSTEM + \"\\n\\n\" + user_presult_prompt\n",
    "    client = load_client()\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            \n",
    "            resp = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=full_prompt,\n",
    "                config={\"temperature\": TEMPERATURE},\n",
    "            )\n",
    "            text = (resp.text or \"\").strip()\n",
    "            # Sometimes models wrap JSON with ```...```. Extract if needed.\n",
    "            #m = re.search(r\"\\{.*\\}\\s*$\", text, re.S)\n",
    "            #logger.info(text)\n",
    "            if text.startswith(\"```json\"):\n",
    "                text = text[7:-3]\n",
    "            payload = json.loads(text)\n",
    "            # Light validation\n",
    "            _ = payload[\"verdict\"]\n",
    "            _ = payload[\"scores\"]\n",
    "            return payload\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "            time.sleep(1.2 * (attempt + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63b7377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cases(root: str):\n",
    "    \"\"\"\n",
    "    Returns list of dicts:\n",
    "    {evaluator, path}\n",
    "    evaluator = subfolder name if present; otherwise 'default'.\n",
    "    \"\"\"\n",
    "    root_path = pathlib.Path(root)\n",
    "    assert root_path.exists(), f\"Path not found: {root}\"\n",
    "    cases = []\n",
    "    for f in root_path.rglob(\"*.json\"):\n",
    "        cases.append({\"evaluator\": \"default\", \"path\": str(f)})\n",
    "    return cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4db5533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _agg_mean(s):\n",
    "    \"\"\"\n",
    "    Calculates the mean of a series after robustly cleaning and converting to numeric type.\n",
    "\n",
    "    Args:\n",
    "        s (Union[pd.Series, list, tuple]): The data series or list of values \n",
    "                                           for which the mean should be calculated.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated mean, rounded to 3 decimal places, or 0.0 if any \n",
    "               exception occurs during processing (e.g., all values are non-numeric).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return round(float(pd.Series(s).dropna().astype(float).mean()), 3)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def _make_leaderboard(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a structured leaderboard DataFrame by aggregating evaluation scores.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing raw evaluation results.\n",
    "                           It is expected to have columns: 'evaluator', 'file',\n",
    "                           'verdict', 'coverage', 'correctness', 'helpfulness',\n",
    "                           'structure', 'rewrite_quality', and 'gold_accuracy'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A leaderboard, sorted in descending order primarily by\n",
    "                      'pass_rate', then by 'avg_correctness', and finally by\n",
    "                      'avg_coverage'. The returned columns include:\n",
    "                      - 'evaluator'\n",
    "                      - 'num_cases' (count of cases evaluated)\n",
    "                      - 'pass_rate' (percentage of 'pass' verdicts)\n",
    "                      - 'avg_coverage', 'avg_correctness', 'avg_helpfulness',\n",
    "                        'avg_structure', 'avg_rewrite_quality' (all calculated mean scores)\n",
    "                      - 'gold_accuracy' (TBD, currently aggregated by _agg_mean)\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"evaluator\",\"num_cases\",\"pass_rate\",\"avg_coverage\",\n",
    "                     \"avg_correctness\",\"avg_helpfulness\",\"avg_structure\",\n",
    "                     \"avg_rewrite_quality\",\"gold_accuracy\"]\n",
    "        )\n",
    "    lb = (\n",
    "        df.groupby(\"evaluator\")\n",
    "             .agg(\n",
    "                 num_cases=(\"file\", \"count\"),\n",
    "                 pass_rate=(\"verdict\", lambda s: round(100.0 * (s == \"pass\").mean(), 2)),\n",
    "                 avg_coverage=(\"coverage\", _agg_mean),\n",
    "                 avg_correctness=(\"correctness\", _agg_mean),\n",
    "                 avg_helpfulness=(\"helpfulness\", _agg_mean),\n",
    "                 avg_structure=(\"structure\", _agg_mean),\n",
    "                 avg_rewrite_quality=(\"rewrite_quality\", _agg_mean),\n",
    "             )\n",
    "             .reset_index()\n",
    "             .sort_values([\"pass_rate\",\"avg_correctness\",\"avg_coverage\"], ascending=False)\n",
    "    )\n",
    "    return lb\n",
    "\n",
    "def evaluate_folder(\n",
    "    root: str,\n",
    "    judge_fn,                      # e.g., call_gemini_judge\n",
    "    save_csv: str = None,\n",
    "    save_json: str = None,\n",
    "    show_progress: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run LLM-as-a-Judge over all JSON files found by find_cases(root).\n",
    "\n",
    "    Returns:\n",
    "        df_records: per-file flat records (DataFrame)\n",
    "        leaderboard: aggregated metrics by evaluator (DataFrame)\n",
    "        details: per-file rich dicts suitable for inspection or saving (list)\n",
    "    \"\"\"\n",
    "    cases = find_cases(root)\n",
    "    logger.info(f\"Found {len(cases)} JSON files in {root}.\")\n",
    "\n",
    "    records, details = [], []\n",
    "\n",
    "    iterator = tqdm(cases) if show_progress else cases\n",
    "    for c in iterator:\n",
    "        try:\n",
    "            with open(c[\"path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "                obj = json.load(f)\n",
    "        except Exception as e:\n",
    "            logger.info(f\"[WARN] Failed to read {c['path']}: {e}\")\n",
    "            continue\n",
    "\n",
    "        standards = obj.get(\"standards\", \"\")\n",
    "        listing = obj.get(\"listing\", \"\")\n",
    "        actual_output = obj.get(\"actual_output\", \"\")\n",
    "\n",
    "        if not (standards and listing and actual_output):\n",
    "            logger.info(f\"[WARN] Missing fields in {c['path']}\")\n",
    "            continue\n",
    "\n",
    "        # Call your judge function with safety net\n",
    "        try:\n",
    "            judge = judge_fn(standards, listing, actual_output)\n",
    "        except Exception as e:\n",
    "            # Record an error verdict so you can spot failures without breaking the run\n",
    "            judge = {\n",
    "                \"verdict\": \"error\",\n",
    "                \"scores\": {\n",
    "                    \"coverage\": None, \"correctness\": None, \"helpfulness\": None,\n",
    "                    \"structure\": None, \"rewrite_quality\": None\n",
    "                },\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "\n",
    "        details.append({\n",
    "            \"evaluator\": c[\"evaluator\"],\n",
    "            \"file\": c[\"path\"],\n",
    "            \"judge\": judge,\n",
    "        })\n",
    "\n",
    "        scores = judge.get(\"scores\", {}) or {}\n",
    "        records.append({\n",
    "            \"evaluator\": c[\"evaluator\"],\n",
    "            \"file\": c[\"path\"],\n",
    "            \"verdict\": judge.get(\"verdict\"),\n",
    "            \"coverage\": scores.get(\"coverage\"),\n",
    "            \"correctness\": scores.get(\"correctness\"),\n",
    "            \"helpfulness\": scores.get(\"helpfulness\"),\n",
    "            \"structure\": scores.get(\"structure\"),\n",
    "            \"rewrite_quality\": scores.get(\"rewrite_quality\"),\n",
    "            \"improvements\":scores.get(\"improvements\")\n",
    "            #\"gold_verdict\": gold\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    leaderboard = _make_leaderboard(df)\n",
    "\n",
    "    if save_csv:\n",
    "        leaderboard.to_csv(save_csv, index=False)\n",
    "    if save_json:\n",
    "        with open(save_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(details, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return df, leaderboard, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a145579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 JSON files in data/validation_original_prompt.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace2dc6095a8479aa5b4a06f742ebf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using your Gemini judge function\n",
    "df_a, lb_a, details_a = evaluate_folder(\n",
    "    root=\"data/validation_original_prompt\",\n",
    "    judge_fn=call_gemini_judge,              # or call_gemini_judge_safe\n",
    "    save_csv=\"data/reporting/leaderboard_original.csv\",\n",
    "    save_json=\"data/reporting/details_original.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e5702",
   "metadata": {},
   "source": [
    "The final leaderboard for the original prompt shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0388ea21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluator</th>\n",
       "      <th>num_cases</th>\n",
       "      <th>pass_rate</th>\n",
       "      <th>avg_coverage</th>\n",
       "      <th>avg_correctness</th>\n",
       "      <th>avg_helpfulness</th>\n",
       "      <th>avg_structure</th>\n",
       "      <th>avg_rewrite_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>24</td>\n",
       "      <td>83.33</td>\n",
       "      <td>4.958</td>\n",
       "      <td>4.958</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  evaluator  num_cases  pass_rate  avg_coverage  avg_correctness  \\\n",
       "0   default         24      83.33         4.958            4.958   \n",
       "\n",
       "   avg_helpfulness  avg_structure  avg_rewrite_quality  \n",
       "0                5              5                4.875  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd5b98",
   "metadata": {},
   "source": [
    "## Improved prompt\n",
    "\n",
    "- **Enhanced Persona and Authority**: It assigns the LLM the role of a **Senior Compliance and Content Auditor specializing in UK financial listings (FCA regulations)**. This demands a stricter, more professional, and regulatory-focused evaluation.\n",
    "\n",
    "- **Mandatory Comprehensive Check**: It requires the model to **systematically evaluate against all six main sections (1-6)** of the standards, preventing premature stopping after finding initial violations.\n",
    "\n",
    "- **Quantitative Analysis Requirements**: The prompt forces objective data collection and reporting for specific standards:\n",
    "\n",
    "    - Calculate and note the **exact word count** of the entire listing.\n",
    "\n",
    "    - For Language/Clarity issues (Standard 2.1), calculate the **sentence length** and check for **active voice**.\n",
    "\n",
    "- **Strict Output Fidelity**: It enforces a precise JSON schema, including the new quant_data field, which must contain the measurable data (e.g., \"Sentence Length: [X] words\").\n",
    "\n",
    "- **Prioritization**: It instructs the model to prioritize violations in **Section 1 (Regulatory)** and issues affecting the critical **first 100 words** of the listing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2aab0e",
   "metadata": {},
   "source": [
    "**Re-running Validation**: The `process_listings` function is called again with the `improved_prompt` and a new output path (`data/validation_improved_prompt/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5c6a0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API validation for 24 listings...\n",
      "\n",
      "--- Processing Listing 1/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 1 saved.\n",
      "\n",
      "--- Processing Listing 2/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 2 saved.\n",
      "\n",
      "--- Processing Listing 3/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 3 saved.\n",
      "\n",
      "--- Processing Listing 4/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 4 saved.\n",
      "\n",
      "--- Processing Listing 5/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 5 saved.\n",
      "\n",
      "--- Processing Listing 6/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 6 saved.\n",
      "\n",
      "--- Processing Listing 7/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 7 saved.\n",
      "\n",
      "--- Processing Listing 8/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 8 saved.\n",
      "\n",
      "--- Processing Listing 9/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 9 saved.\n",
      "\n",
      "--- Processing Listing 10/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 10 saved.\n",
      "\n",
      "--- Processing Listing 11/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 11 saved.\n",
      "\n",
      "--- Processing Listing 12/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 12 saved.\n",
      "\n",
      "--- Processing Listing 13/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 13 saved.\n",
      "\n",
      "--- Processing Listing 14/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 14 saved.\n",
      "\n",
      "--- Processing Listing 15/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 15 saved.\n",
      "\n",
      "--- Processing Listing 16/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 16 saved.\n",
      "\n",
      "--- Processing Listing 17/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 17 saved.\n",
      "\n",
      "--- Processing Listing 18/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 18 saved.\n",
      "\n",
      "--- Processing Listing 19/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 19 saved.\n",
      "\n",
      "--- Processing Listing 20/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 20 saved.\n",
      "\n",
      "--- Processing Listing 21/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 21 saved.\n",
      "\n",
      "--- Processing Listing 22/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 22 saved.\n",
      "\n",
      "--- Processing Listing 23/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 23 saved.\n",
      "\n",
      "--- Processing Listing 24/24 ---\n",
      "  Attempt 1: Calling Gemini...\n",
      "  SUCCESS! Response received.\n",
      "Wrote data/listings.jsonl\n",
      "  Results for Listing 24 saved.\n",
      "\n",
      "--- Validation Loop Complete ---\n"
     ]
    }
   ],
   "source": [
    "improved_prompt = read_txt('prompts/improved_marketplace_validator_prompt.txt')\n",
    "process_listings(listings, improved_prompt, output_filepath = \"data/validation_improved_prompt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294df87e",
   "metadata": {},
   "source": [
    "**Re-running Judge Evaluation**: The `evaluate_folder` function is run on the new output folder, generating the `lb_i` leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f98d66b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 JSON files in data/validation_improved_prompt.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1da1409a7d4edbb8d8d791bbe085f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using your Gemini judge function\n",
    "df_i, lb_i, details_i = evaluate_folder(\n",
    "    root=\"data/validation_improved_prompt\",\n",
    "    judge_fn=call_gemini_judge,              # or call_gemini_judge_safe\n",
    "    save_csv=\"data/reporting/leaderboard_improved.csv\",\n",
    "    save_json=\"data/reporting/details_improved.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13464653",
   "metadata": {},
   "source": [
    "**Improved Prompt Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2682ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evaluator</th>\n",
       "      <th>num_cases</th>\n",
       "      <th>pass_rate</th>\n",
       "      <th>avg_coverage</th>\n",
       "      <th>avg_correctness</th>\n",
       "      <th>avg_helpfulness</th>\n",
       "      <th>avg_structure</th>\n",
       "      <th>avg_rewrite_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>24</td>\n",
       "      <td>33.33</td>\n",
       "      <td>4.458</td>\n",
       "      <td>4.667</td>\n",
       "      <td>4.958</td>\n",
       "      <td>4.708</td>\n",
       "      <td>4.917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  evaluator  num_cases  pass_rate  avg_coverage  avg_correctness  \\\n",
       "0   default         24      33.33         4.458            4.667   \n",
       "\n",
       "   avg_helpfulness  avg_structure  avg_rewrite_quality  \n",
       "0            4.958          4.708                4.917  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe95d3",
   "metadata": {},
   "source": [
    "## Comparison of Original vs. Improved Prompt Performance\n",
    "\n",
    "| Metric | Original Prompt | Improved Prompt | Difference (Improved - Original) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Pass Rate** | 83.33% | 33.33% | -50.00% |\n",
    "| **Avg. Coverage** | 4.958 | 4.458 | -0.500 |\n",
    "| **Avg. Correctness** | 4.958 | 4.667 | -0.291 |\n",
    "| **Avg. Helpfulness** | 5.000 | 4.958 | -0.042 |\n",
    "| **Avg. Structure** | 5.000 | 4.708 | -0.292 |\n",
    "| **Avg. Rewrite Quality** | 4.875 | 4.917 | +0.042 |\n",
    "\n",
    "**Strictness vs. Accuracy**: The most significant change is the dramatic **drop in the Pass Rate from 83.33% to 33.33%**. This suggests the **Improved Prompt** is significantly **stricter** in its validation criteria. It likely instructs the model to adhere to the Marketplace Standards with less tolerance for ambiguity or minor deviations, causing it to fail more listings.\n",
    "\n",
    "**Tradeoff**: **Pass Rate vs. Quality**: The drop in avg_coverage, avg_correctness, and avg_structure suggests that for the cases that did fail, the judge found the **actual output** (the validation result from the initial model call) to be slightly less comprehensive or structured when compared to the (likely stricter) **standards** embedded in the Improved Prompt.\n",
    "\n",
    "**Tradeoff**: **Rewrite Quality**: Interestingly, the **Average Rewrite Quality** slightly **improved** (4.875 to 4.917). This indicates that while the Improved Prompt resulted in more listings failing the overall check, the **suggested changes or rewrites** provided by the model for those failures were rated slightly higher by the LLM-as-a-Judge. This is a common pattern where a stricter prompt leads to a lower pass rate but higher quality corrections when a failure occurs.\n",
    "\n",
    "**Overall Conclusion**: The Improved Validator Prompt trades off a higher overall pass rate for a **stricter validation** process. If the goal of the marketplace is to have **zero tolerance** for minor standard violations, the Improved Prompt is better because it identifies and flags more issues. If the goal is a rapid, high-throughput validation that only catches major errors, the Original Prompt might be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ffeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenvt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
